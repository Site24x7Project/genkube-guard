# LLM selection
LLM_PROVIDER=ollama          # ollama | hf  (extend later if needed)
LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2  # used when LLM_PROVIDER=hf

# API key for hosted providers (required for hf)
LLM_API_KEY=YOUR_HF_TOKEN

# Ollama host (local runs). The ollama python client respects OLLAMA_HOST.
# leave as default if your Ollama is on localhost:11434
OLLAMA_HOST=http://127.0.0.1:11434
